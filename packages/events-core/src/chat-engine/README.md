# Chat Engine Implementation

This is a complete implementation of the independent chat engine according to the `ç¨ç«‹Chatå¼•æ“é‡æ§‹æ–¹æ¡ˆv2_1.md` specification.

## âœ… Completed Features

### Core Architecture
- **`ChatClient`** - Main execution engine with recursive conversation flow (exactly as per gemini-cli)
- **`ChatSession`** - Session management with history, compression, and API handling
- **`Turn`** - Single turn processing with event conversion and tool call handling
- **`ContentGenerator`** - AI content generation abstraction (Google AI + Mock implementations)
- **`ChatEngineConfig`** - Unified configuration management

### Key Capabilities

#### ğŸ”„ Recursive Conversation Logic
- Implements the exact recursive pattern from gemini-cli
- `ChatClient.sendMessageStream` calls itself recursively based on `checkNextSpeaker` results
- Supports both Chat mode (traditional Q&A) and Agent mode (autonomous conversations)

#### ğŸ¯ Mode-Based Control
- **Chat Mode**: `checkNextSpeaker` always returns 'user' â†’ one response then waits
- **Agent Mode**: Uses heuristic analysis to determine if model should continue â†’ autonomous multi-turn conversations

#### ğŸ“¡ Event Streaming
- Complete event system with types: `content`, `thought`, `tool_call_request`, `error`, `user_cancelled`, `max_session_turns_reached`, `chat_compressed`
- Real-time streaming of AI responses
- Proper error handling with retry indicators

#### ğŸ› ï¸ Production Ready
- Full TypeScript type safety using `@google/genai` types
- Google Generative AI integration with fallback to intelligent mock responses
- Session management with turn limits and compression
- tRPC router for web integration

## ğŸš€ Usage

### Basic Usage
```typescript
import { ChatClient } from './core/chat-client.js';
import { ChatEngineConfig } from './config/config.js';
import type { SystemConfig, ChatConfig } from './config/types.js';
import type { ChatStreamEvent } from './events/types.js';

const config = new ChatEngineConfig(systemConfig, chatConfig);
await config.initialize();

const client = new ChatClient(config);
const sessionConfig = config.createSessionConfig('session-id', signal);

// Streaming conversation
for await (const event of client.sendMessageStream(
  'Hello!',
  chatConfig,
  sessionConfig
)) {
  if (event.type === 'content') {
    console.log(event.value);
  }
}
```

### Demo Scripts
- `pnpm chat-engine-demo` - Complete demo with mock responses
- `pnpm test-with-api` - Test with real Google Gemini API (requires GEMINI_API_KEY)

### Configuration
- Set `GEMINI_API_KEY` environment variable to use real Google AI
- Without API key, falls back to intelligent mock responses

## ğŸ“ File Structure
```
src/chat-engine/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.ts              # ChatEngineConfig class
â”‚   â””â”€â”€ types.ts               # Type definitions (uses @google/genai types)
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ chat-client.ts         # Main ChatClient with recursive logic
â”‚   â”œâ”€â”€ chat-session.ts        # Session management
â”‚   â”œâ”€â”€ turn.ts                # Single turn processing
â”‚   â”œâ”€â”€ content-generator.ts   # AI abstraction layer
â”‚   â””â”€â”€ next-speaker-checker.ts # AI-driven continuation logic
â”œâ”€â”€ events/
â”‚   â””â”€â”€ types.ts               # Stream event definitions
â”œâ”€â”€ index.ts                   # Main exports
â”œâ”€â”€ demo.ts                    # Demo script
â””â”€â”€ README.md                  # This file
```

## ğŸ”§ Integration

### tRPC Router
Located at `src/server/routers/chat-engine-router.ts`, provides:
- `sendMessageStream` - Streaming conversation endpoint
- `generateJson` - Structured output generation
- `resetChat` - Session reset

### Type Safety
All types are properly exported and use official Google Generative AI types for maximum compatibility.

## âœ¨ Key Innovation

The recursive conversation pattern enables seamless mode switching:

**Chat Mode Flow:**
```
User: "What is TypeScript?"
Model: "TypeScript is..."
checkNextSpeaker() â†’ 'user' (stops, waits for user)
```

**Agent Mode Flow:**
```
User: "Help me build a web app"
Model: "I'll break this down. First..."
checkNextSpeaker() â†’ 'model' (continues)
Fake User: "Please continue."
Model: "Now for the backend..."
checkNextSpeaker() â†’ 'model' (continues)
...continues until stop condition or limit...
```

This provides natural autonomous behavior while maintaining full control over conversation flow.